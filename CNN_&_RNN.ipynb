{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmkqTa5Ai3OgSMFoIgG2ns",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davids048/ELEC-378/blob/main/CNN_%26_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PCnkW8VYtaSU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "from scipy.fft import fft\n",
        "import scipy\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Sklearn functions are useful for generating train/test splits, and metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "import sklearn\n",
        "\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Librosa is likely the first feature extraction library you should try out. \n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuuvtHv2uztW",
        "outputId": "c5ad381e-fc5f-49f8-826e-f9b737f5948a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data import, don't need to do this again unless need to debug, basically because running these code once will save them to a file, next time just run the code in the next section"
      ],
      "metadata": {
        "id": "TT3LH8pjx7Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Spring 2023/ELEC 378'\n",
        "%ls\n",
        "%cd './kaggle_data/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtQjolnattcm",
        "outputId": "4f42aeb0-bf9c-4630-96c5-2e5ece022bf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Spring 2023/ELEC 378\n",
            "'CNN & RNN.ipynb'  \u001b[0m\u001b[01;34m'Homework 8'\u001b[0m/  \u001b[01;34m'HW 6'\u001b[0m/                 rf_predictions.csv\n",
            " Datas             \u001b[01;34m'Homework 9'\u001b[0m/   \u001b[01;34mkaggle_data\u001b[0m/\n",
            "\u001b[01;34m'Homework 7'\u001b[0m/       \u001b[01;34mHW-4\u001b[0m/         'Random Forest.ipynb'\n",
            "/content/drive/MyDrive/Spring 2023/ELEC 378/kaggle_data/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_conversion = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "emo_labels = []\n",
        "onehot_emo_labels = []\n",
        "\n",
        "audiolist = []\n",
        "nums = []\n",
        "calms = 150\n",
        "clm = 0\n",
        "\n",
        "# iterate through all file\n",
        "points = -1\n",
        "cnt = 0\n",
        "for file in os.listdir():\n",
        "  if cnt == points and points != -1:\n",
        "    print(\"breaking\")\n",
        "    break\n",
        "  # Check whether file is in text format or not\n",
        "  if file.endswith(\".wav\"):\n",
        "    if 'sample' not in file:\n",
        "      # grab the emotion\n",
        "      class_lbl = label_conversion.index(file[:-7])\n",
        "      # one-hot encoding\n",
        "      if class_lbl == 1:\n",
        "        clm += 1\n",
        "      if clm <= calms or class_lbl != 1:\n",
        "        onehot_emo_labels.append([0]*class_lbl+[1]+[0]*(8-class_lbl-1))\n",
        "        # no one-hot\n",
        "        emo_labels.append(class_lbl)\n",
        "    #print(1)\n",
        "    # sample = 6 so for this case it's 6\n",
        "    if clm <= calms or class_lbl != 1:\n",
        "      nums.append(file[:-4]) # goes to emotion + num (removes .wav)\n",
        "      # fs, audiofile = wavfile.read(file)\n",
        "      audiofile, sr = librosa.load(file, sr=48000)\n",
        "\n",
        "      # add audio as-is to array\n",
        "      audiolist.append(audiofile)\n",
        "      cnt += 1\n",
        "if points == -1: points = cnt"
      ],
      "metadata": {
        "id": "M2friAbMu4op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "626e87e0-70fc-48c1-c744-8d5cf51dc5da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-58fcf69615eb>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"breaking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all data to mono: These data are already normalized if its read using librosa.load\n",
        "new_audiolist = []\n",
        "for audio in audiolist:\n",
        "  audio_mono = librosa.to_mono(audio)\n",
        "  new_audiolist.append(audio_mono)\n",
        "audiolist = new_audiolist"
      ],
      "metadata": {
        "id": "Jysf-LLjw8zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find min len of all audio files\n",
        "min_length = float('inf')\n",
        "for audio in audiolist:\n",
        "  min_length = min(min_length, audio.shape[0])"
      ],
      "metadata": {
        "id": "sgyTE2sdw-8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickel audio, emo_list, and minlenth\n",
        "Datas = (audiolist, emo_labels, min_length)\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Spring 2023/ELEC 378/Datas', 'wb') as fp:\n",
        "  pickle.dump(Datas, fp)"
      ],
      "metadata": {
        "id": "qC49qXNexCK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data (new tool)\n"
      ],
      "metadata": {
        "id": "KHA-dd4QyFQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Spring 2023/ELEC 378/Datas\", \"rb\") as fp:   # Unpickling\n",
        "  data_tupel = pickle.load(fp)"
      ],
      "metadata": {
        "id": "HbZf4hn-xo_u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios = data_tupel[0]\n",
        "emo_labels = data_tupel[1]\n",
        "min_len = data_tupel[2]"
      ],
      "metadata": {
        "id": "xZjVPFRdyAx_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioData:\n",
        "  def __init__(self, audios, emo_labels, min_len):\n",
        "    self.audios = audios\n",
        "    self.emo_labels = emo_labels\n",
        "    self.min_len = min_len\n",
        "    self.sr = 48000\n",
        "\n",
        "  # make feature matrices:\n",
        "  def make_mfcc_mat(self):\n",
        "    n_mfcc = 20\n",
        "    # Initialize lists to store the feature matrices and labels\n",
        "    mfccs = []\n",
        "    for audio in self.audios:\n",
        "      mfcc = librosa.feature.mfcc(y=audio[:self.min_len], hop_length = 512, n_mels=128, n_fft = 2048,sr=self.sr, n_mfcc=n_mfcc)\n",
        "      # mfcc = np.ravel(mfcc)\n",
        "      # Append the feature matrices and label to the lists\n",
        "      mfccs.append(mfcc)\n",
        "    print(len(mfccs))\n",
        "    # Concatenate the feature matrices along the time axis to create feature vectors\n",
        "    mfccs_mat = np.stack(mfccs)\n",
        "    print(mfccs_mat.shape)\n",
        "    return mfccs_mat\n",
        "  \n",
        "  def make_chroma_mat(self):\n",
        "    n_chroma = 12\n",
        "    chromas = []\n",
        "    for audio in self.audios:\n",
        "      chroma = librosa.feature.chroma_stft(y=audio[:self.min_len], sr=self.sr, n_chroma=n_chroma)\n",
        "      chroma = np.ravel(chroma)\n",
        "      chromas.append(chroma)\n",
        "    chromas_mat = np.vstack(chromas)\n",
        "    return chromas_mat\n",
        "  \n",
        "  def make_contrast_mat(self):\n",
        "    n_contrast = 6\n",
        "    contrasts = []\n",
        "    for audio in self.audios:\n",
        "      contrast = librosa.feature.spectral_contrast(y=audio[:self.min_len], sr=self.sr, n_bands=n_contrast)\n",
        "      contrast = np.ravel(contrast)\n",
        "      contrasts.append(contrast)\n",
        "    contrasts_mat = np.vstack(contrasts)\n",
        "    return contrasts_mat\n",
        "\n",
        "  def make_rmse_mat(self):\n",
        "    rmses = []\n",
        "    for audio in self.audios:\n",
        "      energy = librosa.feature.rmse(y=audio)\n",
        "      print(energy.shape)\n",
        "      energy = np.ravel(energy)\n",
        "      rmses.append(energy)\n",
        "    rmse_mat = np.vstack(rmses)\n",
        "    return rmse_mat\n",
        "\n"
      ],
      "metadata": {
        "id": "WEJDAV7iywzr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this class, first create a class object, then create features 1 by 1 ( recommended) so that later we can combine features as we want. If we want to change params, just change the functions\n",
        "\n"
      ],
      "metadata": {
        "id": "bIUCyZx27kqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a data object\n",
        "audio_obj = AudioData(audios, emo_labels,min_len)"
      ],
      "metadata": {
        "id": "j5knixkS31Bo"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get features\n",
        "# feature1 = audio_obj.make_mfcc_mat()\n",
        "# feature2 = audio_obj.make_chroma_mat()\n",
        "# feature3 = audio_obj.make_contrast_mat()\n",
        "feature4 = audio_obj.make_rmse_mat()"
      ],
      "metadata": {
        "id": "NTAbsdzH4A8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ad4edb-d967-410f-f8cb-23f2c915dae1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1125\n",
            "(1125, 20, 276)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature1.shape)\n",
        "print(type(feature1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMWh4twuGRY",
        "outputId": "5a698eac-d0cd-45fd-fb9d-0c14c88f2807"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22500, 276)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack the feature vectors horizontally to create the final feature matrix\n",
        "# X = np.hstack([feature1, feature2, feature3])\n",
        "X=feature1"
      ],
      "metadata": {
        "id": "F_nHlxaY4fJH"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(feature1.shape)\n",
        "# print(feature2.shape)\n",
        "# print(feature3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF4YyB0MOFHA",
        "outputId": "ed50cbd5-ca5f-40a0-b043-1dc1d15ca394"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1125, 20, 276)\n",
            "(1125, 20, 276)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset"
      ],
      "metadata": {
        "id": "hINjZ1D6MYBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, audio_obj.emo_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hwkQfEUZMXe_"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx3c47lgP317",
        "outputId": "656ccd5c-6aa6-4692-c48f-9e8c8bb824bd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 7, 5, 4, 5, 6, 2, 4, 7, 5, 1, 3, 0, 4, 0, 0, 7, 7, 6, 5, 2, 2, 2, 2, 2, 0, 1, 6, 0, 7, 2, 3, 2, 4, 3, 0, 4, 2, 7, 3, 6, 7, 7, 3, 2, 1, 2, 1, 1, 1, 6, 2, 1, 0, 3, 1, 5, 3, 7, 3, 4, 7, 2, 4, 6, 0, 6, 7, 4, 7, 3, 1, 4, 1, 6, 6, 6, 2, 6, 3, 3, 1, 4, 7, 1, 3, 4, 4, 6, 3, 0, 2, 3, 5, 4, 7, 2, 4, 6, 7, 5, 0, 7, 1, 1, 2, 2, 7, 3, 3, 7, 6, 4, 7, 0, 5, 1, 3, 6, 0, 3, 2, 7, 3, 4, 2, 2, 3, 4, 2, 3, 6, 0, 2, 5, 6, 1, 1, 5, 0, 0, 2, 2, 4, 6, 7, 3, 1, 1, 3, 1, 0, 4, 0, 7, 7, 5, 4, 3, 4, 0, 2, 7, 6, 1, 1, 6, 0, 2, 4, 4, 1, 3, 0, 2, 4, 2, 4, 1, 0, 4, 5, 1, 6, 4, 2, 0, 3, 4, 0, 4, 5, 2, 6, 3, 1, 3, 4, 7, 7, 5, 4, 2, 7, 2, 4, 7, 1, 7, 4, 2, 0, 0, 5, 7, 1, 0, 4, 1, 6, 0, 4, 0, 0, 3, 2, 0, 3, 5, 2, 2, 4, 7, 7, 7, 6, 6, 6, 0, 6, 6, 0, 7, 4, 0, 0, 3, 6, 7, 0, 1, 2, 4, 7, 2, 7, 1, 1, 2, 4, 5, 4, 4, 1, 4, 1, 5, 4, 4, 2, 3, 4, 6, 4, 0, 0, 7, 0, 3, 3, 3, 2, 0, 3, 7, 5, 7, 6, 1, 3, 2, 4, 1, 0, 1, 4, 6, 2, 1, 1, 2, 4, 2, 7, 4, 4, 6, 4, 2, 7, 2, 0, 4, 1, 6, 2, 5, 1, 4, 6, 7, 4, 1, 7, 2, 2, 6, 2, 5, 6, 1, 1, 1, 6, 4, 6, 4, 3, 0, 7, 0, 6, 7, 3, 6, 7, 3, 3, 2, 6, 6, 3, 3, 2, 0, 5, 7, 1, 2, 0, 2, 6, 3, 7, 5, 5, 4, 7, 6, 3, 3, 0, 7, 3, 3, 1, 0, 4, 5, 1, 3, 4, 4, 1, 0, 4, 6, 7, 6, 0, 3, 5, 7, 1, 1, 3, 4, 7, 5, 3, 4, 0, 0, 7, 6, 2, 3, 1, 6, 2, 7, 7, 2, 1, 1, 4, 0, 3, 6, 0, 4, 7, 3, 3, 5, 2, 2, 6, 1, 5, 1, 6, 3, 7, 3, 5, 2, 1, 7, 4, 3, 1, 2, 2, 4, 0, 4, 1, 7, 0, 4, 4, 7, 0, 6, 1, 1, 3, 1, 1, 5, 0, 5, 6, 6, 1, 4, 3, 2, 5, 1, 3, 1, 1, 0, 7, 6, 7, 2, 7, 3, 6, 7, 7, 2, 3, 7, 2, 1, 1, 7, 0, 5, 6, 1, 3, 0, 2, 3, 5, 3, 6, 3, 0, 5, 0, 7, 7, 3, 4, 7, 0, 0, 2, 2, 0, 6, 7, 4, 1, 5, 6, 3, 4, 5, 6, 4, 7, 6, 0, 1, 3, 2, 7, 1, 1, 1, 0, 6, 4, 1, 2, 7, 7, 7, 7, 7, 6, 4, 3, 1, 4, 2, 7, 2, 0, 4, 1, 1, 4, 4, 7, 6, 4, 2, 7, 6, 2, 0, 7, 0, 3, 0, 3, 2, 0, 5, 6, 1, 3, 3, 1, 5, 0, 7, 7, 1, 0, 0, 4, 0, 0, 6, 4, 1, 5, 0, 1, 6, 5, 7, 2, 7, 6, 7, 1, 2, 6, 1, 3, 3, 7, 5, 7, 6, 1, 1, 1, 3, 1, 2, 0, 5, 0, 7, 1, 7, 0, 3, 6, 2, 2, 7, 4, 4, 7, 7, 7, 2, 5, 2, 3, 1, 7, 6, 6, 0, 0, 4, 7, 4, 6, 6, 0, 2, 2, 7, 3, 7, 0, 5, 6, 3, 3, 3, 1, 1, 0, 4, 2, 6, 0, 2, 4, 7, 6, 2, 2, 0, 3, 1, 1, 2, 3, 6, 2, 3, 7, 7, 0, 7, 1, 1, 6, 3, 7, 0, 6, 6, 0, 2, 5, 1, 7, 0, 1, 4, 0, 7, 3, 0, 6, 4, 2, 0, 3, 4, 7, 1, 5, 2, 3, 4, 5, 3, 6, 5, 0, 2, 7, 7, 6, 2, 6, 3, 0, 5, 3, 2, 6, 4, 4, 7, 2, 3, 3, 2, 0, 0, 3, 3, 4, 5, 0, 0, 6, 3, 4, 4, 0, 7, 5, 7, 4, 4, 5, 4, 6, 1, 2, 6, 7, 1, 4, 1, 4, 1, 5, 6, 0, 0, 4, 6, 7, 1, 4, 2, 6, 6, 4, 6, 3, 7, 4, 6, 0, 2, 3, 6, 7, 3, 6, 0, 5, 4, 7, 6, 5, 1, 7, 3, 4, 7, 6, 2, 1, 0, 0, 2, 3, 3, 3, 3, 5, 5, 4, 1, 0, 4, 3, 1, 6, 2, 6, 7, 2, 6, 2, 0, 3, 6, 2, 7, 2, 1, 6, 3, 6, 4, 0, 0, 2, 1, 6, 4, 1, 1, 7, 4, 6, 2, 0, 2, 4, 3, 1, 6, 0, 5, 7, 4, 0, 5, 7, 2, 7, 5, 4, 0, 3, 6, 6, 2, 0, 3, 2, 7, 3, 6, 3, 6, 2, 6, 1, 2, 3, 1, 0, 0, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a data loader\n"
      ],
      "metadata": {
        "id": "GHxnl61h8S2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class AudioDataSet(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    x = self.X[index]\n",
        "    y = self.y[index]\n",
        "\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.long).unsqueeze(0)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "073jet0p8SYc"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AudioDataSet(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "wFp3CQj_5Hk5"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = AudioDataSet(X_test, y_test)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "WSE2V0_IbSSP"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZXp2Y2Lhv5P",
        "outputId": "f464c32e-4568-4d8c-a18f-5a5ee1ebbece"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(900, 20, 276)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    # Define the layers of the network\n",
        "    self.conv1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "    self.fc1 = nn.Linear(in_features=345, out_features=128)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    print(x.shape)\n",
        "    x = self.relu1(x)\n",
        "    print(x.shape)\n",
        "    x = self.pool1(x)\n",
        "    print(x.shape)\n",
        "    x = self.conv2(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.fc2(x)\n",
        "    print(x.shape)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "2uIFPVe-Tnbi"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "kSSPEyaJaOUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init the model\n",
        "cnn_model = CNN(len(audio_obj.audios), 8)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(5):\n",
        "  for idx, (features, label) in enumerate(train_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    output = cnn_model(features)\n",
        "    loss = criterion(output, label.squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "t6ebZKolZ-dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for data, target in val_dataloader:\n",
        "    target = target.squeeze(dim=1)\n",
        "    # print(target.size())\n",
        "    output = cnn_model(data)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    print(predicted)\n",
        "    total += target.size(0)\n",
        "    correct += torch.sum(torch.eq(predicted, target)).item()\n",
        "    print(torch.sum(torch.eq(predicted, target)).item())\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "rvWWnmeahaBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3ea4WObaXmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}